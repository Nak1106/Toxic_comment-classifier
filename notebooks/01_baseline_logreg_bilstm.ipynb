{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 1: Baseline Models (Logistic Regression & BiLSTM)\n",
        "\n",
        "This notebook establishes baseline performance using traditional machine learning and simple deep learning approaches.\n",
        "\n",
        "## Contents\n",
        "1. Logistic Regression with TF-IDF\n",
        "2. BiLSTM Model\n",
        "3. Baseline Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "from src.config import DATA_DIR, MODELS_DIR, REPORTS_DIR, LABELS\n",
        "from src.data_utils import load_raw_jigsaw, train_valid_test_split, build_dataloaders_rnn, basic_text_clean\n",
        "from src.metrics import compute_classification_metrics\n",
        "from src.training.bilstm_utils import train_bilstm_model, train_epoch_bilstm, eval_epoch_bilstm\n",
        "from src.models.rnn_models import BiLSTMClassifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Split Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = load_raw_jigsaw(DATA_DIR / \"jigsaw_train.csv\")\n",
        "train_df, valid_df, test_df = train_valid_test_split(df)\n",
        "\n",
        "print(f\"Train: {len(train_df)}\")\n",
        "print(f\"Valid: {len(valid_df)}\")\n",
        "print(f\"Test: {len(test_df)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Logistic Regression Baseline\n",
        "\n",
        "Logistic Regression with TF-IDF features serves as a strong traditional ML baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean text\n",
        "train_texts = [basic_text_clean(t) for t in train_df[\"comment_text\"].tolist()]\n",
        "valid_texts = [basic_text_clean(t) for t in valid_df[\"comment_text\"].tolist()]\n",
        "\n",
        "y_train = train_df[LABELS].values\n",
        "y_valid = valid_df[LABELS].values\n",
        "\n",
        "# TF-IDF vectorization\n",
        "print(\"Fitting TF-IDF vectorizer...\")\n",
        "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), min_df=3)\n",
        "X_train_tfidf = tfidf.fit_transform(train_texts)\n",
        "X_valid_tfidf = tfidf.transform(valid_texts)\n",
        "\n",
        "print(f\"TF-IDF shape: {X_train_tfidf.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train multi-label logistic regression\n",
        "print(\"Training Logistic Regression...\")\n",
        "logreg = MultiOutputClassifier(LogisticRegression(max_iter=100, C=4.0, solver='lbfgs'))\n",
        "logreg.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict\n",
        "y_prob_logreg = np.array([clf.predict_proba(X_valid_tfidf)[:, 1] for clf in logreg.estimators_]).T\n",
        "\n",
        "# Evaluate\n",
        "metrics_logreg = compute_classification_metrics(y_valid, y_prob_logreg, threshold=0.5, label_names=LABELS)\n",
        "\n",
        "print(f\"\\nLogistic Regression Results:\")\n",
        "print(f\"Macro F1: {metrics_logreg['macro_f1']:.4f}\")\n",
        "print(f\"Micro F1: {metrics_logreg['micro_f1']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. BiLSTM Baseline\n",
        "\n",
        "Train a BiLSTM model using our reusable training utilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Train from scratch (uncomment to run full training)\n",
        "# model, vocab, metrics_bilstm = train_bilstm_model(epochs=3)\n",
        "\n",
        "# Option 2: Quick demo with 1 epoch\n",
        "print(\"Training BiLSTM for 1 epoch (demo)...\")\n",
        "train_loader, valid_loader, vocab = build_dataloaders_rnn(train_df, valid_df)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "num_labels = len(LABELS)\n",
        "model_bilstm = BiLSTMClassifier(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=128,\n",
        "    hidden_dim=128,\n",
        "    num_labels=num_labels,\n",
        "    pad_idx=vocab[\"<pad>\"],\n",
        ").to(device)\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model_bilstm.parameters(), lr=1e-3)\n",
        "\n",
        "# Train for 1-2 epochs (quick demo)\n",
        "for epoch in range(1, 3):\n",
        "    train_loss = train_epoch_bilstm(model_bilstm, train_loader, criterion, optimizer, device)\n",
        "    y_true, y_prob_bilstm = eval_epoch_bilstm(model_bilstm, valid_loader, device)\n",
        "    metrics_bilstm = compute_classification_metrics(y_true, y_prob_bilstm, threshold=0.5, label_names=LABELS)\n",
        "    print(f\"Epoch {epoch} - Loss: {train_loss:.4f}, Macro F1: {metrics_bilstm['macro_f1']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Compare Baselines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Per-label comparison\n",
        "comparison_data = []\n",
        "for label in LABELS:\n",
        "    logreg_f1 = metrics_logreg[\"per_label\"][label][\"f1\"]\n",
        "    bilstm_f1 = metrics_bilstm[\"per_label\"][label][\"f1\"]\n",
        "    comparison_data.append({\n",
        "        \"Label\": label,\n",
        "        \"LogReg F1\": logreg_f1,\n",
        "        \"BiLSTM F1\": bilstm_f1,\n",
        "        \"Improvement\": bilstm_f1 - logreg_f1\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\nPer-Label F1 Comparison:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "x = np.arange(len(LABELS))\n",
        "width = 0.35\n",
        "\n",
        "ax.bar(x - width/2, comparison_df[\"LogReg F1\"], width, label='Logistic Regression', color='steelblue')\n",
        "ax.bar(x + width/2, comparison_df[\"BiLSTM F1\"], width, label='BiLSTM', color='coral')\n",
        "\n",
        "ax.set_xlabel('Label')\n",
        "ax.set_ylabel('F1 Score')\n",
        "ax.set_title('Baseline Model Comparison: F1 Score per Label')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(LABELS, rotation=45)\n",
        "ax.legend()\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nOverall Comparison:\")\n",
        "print(f\"LogReg - Macro F1: {metrics_logreg['macro_f1']:.4f}, Micro F1: {metrics_logreg['micro_f1']:.4f}\")\n",
        "print(f\"BiLSTM - Macro F1: {metrics_bilstm['macro_f1']:.4f}, Micro F1: {metrics_bilstm['micro_f1']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Qualitative Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show predictions for a few examples\n",
        "sample_indices = [0, 10, 20]\n",
        "\n",
        "for idx in sample_indices:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Example {idx}:\")\n",
        "    print(f\"Text: {valid_df.iloc[idx]['comment_text'][:150]}...\")\n",
        "    print(f\"\\nTrue labels: {[l for l in LABELS if valid_df.iloc[idx][l] == 1]}\")\n",
        "    print(f\"\\nLogReg predictions:\")\n",
        "    for i, label in enumerate(LABELS):\n",
        "        print(f\"  {label:15s}: {y_prob_logreg[idx, i]:.3f}\")\n",
        "    print(f\"\\nBiLSTM predictions:\")\n",
        "    for i, label in enumerate(LABELS):\n",
        "        print(f\"  {label:15s}: {y_prob_bilstm[idx, i]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "**Key Findings:**\n",
        "1. **Logistic Regression**: Strong traditional baseline, especially for common labels\n",
        "2. **BiLSTM**: Shows improvements with sequence modeling, captures context better\n",
        "3. **Rare Labels**: Both models struggle with `threat` and `identity_hate` due to class imbalance\n",
        "\n",
        "**Next Steps:**\n",
        "- Try transformer models (BERT, DistilBERT) for better context understanding\n",
        "- Add lexicon features to help with rare labels  \n",
        "- Experiment with class balancing techniques\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
