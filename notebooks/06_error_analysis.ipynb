{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.getcwd())\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from src.config import DATA_DIR, MODELS_DIR, REPORTS_DIR, LABELS, MAX_SEQ_LEN\n",
        "from src.data_utils import load_raw_jigsaw, train_valid_test_split, basic_text_clean, build_dataloaders_rnn\n",
        "from src.metrics import compute_classification_metrics\n",
        "from src.models.rnn_models import BiLSTMClassifier, BiLSTMAttentionClassifier\n",
        "from src.models.transformer_models import create_bert_base, create_distilbert, LexiconHybridBert\n",
        "from src.models.contrastive_model import ContrastiveBertEncoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Error Analysis for Toxic Comment Classification\n",
        "\n",
        "In this notebook we go beyond global metrics and look at **how** and **where** our models fail.\n",
        "\n",
        "We focus on the following models:\n",
        "\n",
        "- Classical: Logistic Regression with TF–IDF\n",
        "- Deep baselines: BiLSTM, BiLSTM + Attention\n",
        "- Transformers: DistilBERT, BERT-base\n",
        "- Hybrid: Lexicon-augmented BERT\n",
        "- Representation learning: Contrastive-BERT classifier\n",
        "\n",
        "We will:\n",
        "\n",
        "1. Recreate the validation split used during training.\n",
        "2. Compute predictions for each model on the same validation set.\n",
        "3. Compare macro and per-label F1 scores.\n",
        "4. Inspect typical failure cases:\n",
        "   - False negatives for rare labels (severe_toxic, threat, identity_hate).\n",
        "   - False positives on identity mentions.\n",
        "5. Save prediction arrays so that ensemble analysis can be done separately.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = load_raw_jigsaw(DATA_DIR / \"jigsaw_train.csv\")\n",
        "df[\"clean\"] = df[\"comment_text\"].astype(str).apply(basic_text_clean)\n",
        "train_df, valid_df, test_df = train_valid_test_split(df)\n",
        "print(train_df.shape, valid_df.shape, test_df.shape)\n",
        "valid_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "def get_logreg_predictions(train_df, valid_df):\n",
        "    X_tr = train_df[\"clean\"].tolist()\n",
        "    X_va = valid_df[\"clean\"].tolist()\n",
        "    y_tr = train_df[LABELS].values\n",
        "    y_va = valid_df[LABELS].values\n",
        "    \n",
        "    vec = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.9, sublinear_tf=True)\n",
        "    base = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
        "    clf = make_pipeline(vec, OneVsRestClassifier(base, n_jobs=-1))\n",
        "    clf.fit(X_tr, y_tr)\n",
        "    \n",
        "    y_prob = clf.predict_proba(X_va)\n",
        "    return y_va, np.array(y_prob), clf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_bilstm_predictions(train_df, valid_df):\n",
        "    valid_loader, vocab = None, None\n",
        "    # build_dataloaders_rnn returns (train_loader, valid_loader, vocab)\n",
        "    _, valid_loader, vocab = build_dataloaders_rnn(train_df, valid_df, max_len=100)\n",
        "    \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    checkpoint = torch.load(MODELS_DIR / \"bilstm_baseline.pt\", map_location=device)\n",
        "    model = BiLSTMClassifier(\n",
        "        vocab_size=len(checkpoint[\"vocab\"]),\n",
        "        embed_dim=128,\n",
        "        hidden_dim=128,\n",
        "        num_labels=len(LABELS),\n",
        "        pad_idx=checkpoint[\"vocab\"][\"<pad>\"],\n",
        "    )\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in valid_loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            logits = model(x)\n",
        "            probs = torch.sigmoid(logits)\n",
        "            all_probs.append(probs.cpu().numpy())\n",
        "            all_labels.append(y.cpu().numpy())\n",
        "    \n",
        "    y_true = np.concatenate(all_labels, axis=0)\n",
        "    y_prob = np.concatenate(all_probs, axis=0)\n",
        "    return y_true, y_prob\n",
        "\n",
        "def get_bilstm_attn_predictions(train_df, valid_df):\n",
        "    _, valid_loader, vocab = build_dataloaders_rnn(train_df, valid_df, max_len=100)\n",
        "    \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    checkpoint = torch.load(MODELS_DIR / \"bilstm_attention.pt\", map_location=device)\n",
        "    model = BiLSTMAttentionClassifier(\n",
        "        vocab_size=len(checkpoint[\"vocab\"]),\n",
        "        embed_dim=128,\n",
        "        hidden_dim=128,\n",
        "        num_labels=len(LABELS),\n",
        "        pad_idx=checkpoint[\"vocab\"][\"<pad>\"],\n",
        "    )\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "    all_attn = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in valid_loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            logits, attn = model(x)\n",
        "            probs = torch.sigmoid(logits)\n",
        "            all_probs.append(probs.cpu().numpy())\n",
        "            all_labels.append(y.cpu().numpy())\n",
        "            all_attn.append(attn.cpu().numpy())\n",
        "    \n",
        "    y_true = np.concatenate(all_labels, axis=0)\n",
        "    y_prob = np.concatenate(all_probs, axis=0)\n",
        "    attn = np.concatenate(all_attn, axis=0)\n",
        "    return y_true, y_prob, attn, vocab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class JigsawBertDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels.astype(\"float32\")\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        enc = self.tokenizer(\n",
        "            str(self.texts[idx]),\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float32),\n",
        "        }\n",
        "\n",
        "class JigsawLexiconDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len, lex_feats):\n",
        "        self.texts = texts\n",
        "        self.labels = labels.astype(\"float32\")\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.lex_feats = lex_feats.astype(\"float32\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        enc = self.tokenizer(\n",
        "            str(self.texts[idx]),\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float32),\n",
        "            \"lex_feats\": torch.tensor(self.lex_feats[idx], dtype=torch.float32),\n",
        "        }\n",
        "\n",
        "def get_transformer_predictions(model_name: str, model_type: str, valid_df, with_lexicon=False):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    texts = valid_df[\"comment_text\"].tolist()\n",
        "    y_true = valid_df[LABELS].values\n",
        "    \n",
        "    if model_type in [\"bert\", \"distilbert\"]:\n",
        "        tok_name = \"bert-base-uncased\" if model_type == \"bert\" else \"distilbert-base-uncased\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(tok_name)\n",
        "        ds = JigsawBertDataset(texts, y_true, tokenizer, MAX_SEQ_LEN)\n",
        "        loader = DataLoader(ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "        \n",
        "        if model_type == \"bert\":\n",
        "            model = create_bert_base(len(LABELS))\n",
        "        else:\n",
        "            model = create_distilbert(len(LABELS))\n",
        "        \n",
        "        model.load_state_dict(torch.load(MODELS_DIR / model_name, map_location=device))\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        \n",
        "        all_probs, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in loader:\n",
        "                ids = batch[\"input_ids\"].to(device)\n",
        "                mask = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "                logits = model(ids, mask)\n",
        "                probs = torch.sigmoid(logits)\n",
        "                all_probs.append(probs.cpu().numpy())\n",
        "                all_labels.append(labels.cpu().numpy())\n",
        "        \n",
        "        y_true = np.concatenate(all_labels, axis=0)\n",
        "        y_prob = np.concatenate(all_probs, axis=0)\n",
        "        return y_true, y_prob\n",
        "    \n",
        "    elif model_type == \"lexicon_bert\":\n",
        "        from src.lexicon_utils import extract_lexicon_features\n",
        "        \n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        lex_feats = extract_lexicon_features(texts)\n",
        "        ds = JigsawLexiconDataset(texts, y_true, tokenizer, MAX_SEQ_LEN, lex_feats)\n",
        "        loader = DataLoader(ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "        \n",
        "        model = LexiconHybridBert.from_pretrained(\"bert-base-uncased\", num_labels=len(LABELS), lexicon_dim=lex_feats.shape[1])\n",
        "        model.load_state_dict(torch.load(MODELS_DIR / model_name, map_location=device))\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        \n",
        "        all_probs, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for batch in loader:\n",
        "                ids = batch[\"input_ids\"].to(device)\n",
        "                mask = batch[\"attention_mask\"].to(device)\n",
        "                lex = batch[\"lex_feats\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "                logits = model(ids, mask, lex)\n",
        "                probs = torch.sigmoid(logits)\n",
        "                all_probs.append(probs.cpu().numpy())\n",
        "                all_labels.append(labels.cpu().numpy())\n",
        "        \n",
        "        y_true = np.concatenate(all_labels, axis=0)\n",
        "        y_prob = np.concatenate(all_probs, axis=0)\n",
        "        return y_true, y_prob\n",
        "    \n",
        "    else:\n",
        "        raise ValueError(\"Unsupported transformer type\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.training.train_classifier_from_encoder import ContrastiveClassifier\n",
        "\n",
        "def get_contrastive_predictions(valid_df):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    texts = valid_df[\"comment_text\"].tolist()\n",
        "    y_true = valid_df[LABELS].values\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    \n",
        "    class JigsawEncoderDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, texts, labels, tokenizer, max_len):\n",
        "            self.texts = texts\n",
        "            self.labels = labels.astype(\"float32\")\n",
        "            self.tokenizer = tokenizer\n",
        "            self.max_len = max_len\n",
        "        \n",
        "        def __len__(self): return len(self.texts)\n",
        "        \n",
        "        def __getitem__(self, idx):\n",
        "            enc = self.tokenizer(\n",
        "                str(self.texts[idx]),\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=MAX_SEQ_LEN,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "            return {\n",
        "                \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "                \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "                \"labels\": torch.tensor(self.labels[idx], dtype=torch.float32),\n",
        "            }\n",
        "    \n",
        "    ds = JigsawEncoderDataset(texts, y_true, tokenizer, MAX_SEQ_LEN)\n",
        "    loader = DataLoader(ds, batch_size=16, shuffle=False, num_workers=2)\n",
        "    \n",
        "    encoder = ContrastiveBertEncoder(\"bert-base-uncased\")\n",
        "    encoder.load_state_dict(torch.load(MODELS_DIR / \"contrastive_encoder.pt\", map_location=device))\n",
        "    model = ContrastiveClassifier(encoder, num_labels=len(LABELS))\n",
        "    model.load_state_dict(torch.load(MODELS_DIR / \"contrastive_bert_classifier.pt\", map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    all_probs, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            ids = batch[\"input_ids\"].to(device)\n",
        "            mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            logits = model(ids, mask)\n",
        "            probs = torch.sigmoid(logits)\n",
        "            all_probs.append(probs.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "    \n",
        "    y_true = np.concatenate(all_labels, axis=0)\n",
        "    y_prob = np.concatenate(all_probs, axis=0)\n",
        "    return y_true, y_prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = {}  # model_name -> dict(y_true, y_prob, metrics)\n",
        "\n",
        "# Logistic Regression\n",
        "y_true_lr, y_prob_lr, logreg_model = get_logreg_predictions(train_df, valid_df)\n",
        "results[\"logreg\"] = {\n",
        "    \"y_true\": y_true_lr,\n",
        "    \"y_prob\": y_prob_lr,\n",
        "    \"metrics\": compute_classification_metrics(y_true_lr, y_prob_lr, threshold=0.5, label_names=LABELS),\n",
        "}\n",
        "\n",
        "# BiLSTM\n",
        "y_true_bi, y_prob_bi = get_bilstm_predictions(train_df, valid_df)\n",
        "results[\"bilstm\"] = {\n",
        "    \"y_true\": y_true_bi,\n",
        "    \"y_prob\": y_prob_bi,\n",
        "    \"metrics\": compute_classification_metrics(y_true_bi, y_prob_bi, threshold=0.5, label_names=LABELS),\n",
        "}\n",
        "\n",
        "# BiLSTM + Attention\n",
        "y_true_bia, y_prob_bia, attn_bia, vocab_bia = get_bilstm_attn_predictions(train_df, valid_df)\n",
        "results[\"bilstm_attn\"] = {\n",
        "    \"y_true\": y_true_bia,\n",
        "    \"y_prob\": y_prob_bia,\n",
        "    \"metrics\": compute_classification_metrics(y_true_bia, y_prob_bia, threshold=0.5, label_names=LABELS),\n",
        "}\n",
        "\n",
        "# DistilBERT\n",
        "y_true_db, y_prob_db = get_transformer_predictions(\"distilbert_toxic.pt\", \"distilbert\", valid_df)\n",
        "results[\"distilbert\"] = {\n",
        "    \"y_true\": y_true_db,\n",
        "    \"y_prob\": y_prob_db,\n",
        "    \"metrics\": compute_classification_metrics(y_true_db, y_prob_db, threshold=0.5, label_names=LABELS),\n",
        "}\n",
        "\n",
        "# BERT-base\n",
        "y_true_bert, y_prob_bert = get_transformer_predictions(\"bert_toxic.pt\", \"bert\", valid_df)\n",
        "results[\"bert\"] = {\n",
        "    \"y_true\": y_true_bert,\n",
        "    \"y_prob\": y_prob_bert,\n",
        "    \"metrics\": compute_classification_metrics(y_true_bert, y_prob_bert, threshold=0.5, label_names=LABELS),\n",
        "}\n",
        "\n",
        "# Lexicon-BERT\n",
        "y_true_lex, y_prob_lex = get_transformer_predictions(\"bert_lexicon_hybrid.pt\", \"lexicon_bert\", valid_df)\n",
        "results[\"lexicon_bert\"] = {\n",
        "    \"y_true\": y_true_lex,\n",
        "    \"y_prob\": y_prob_lex,\n",
        "    \"metrics\": compute_classification_metrics(y_true_lex, y_prob_lex, threshold=0.5, label_names=LABELS),\n",
        "}\n",
        "\n",
        "# Contrastive-BERT classifier\n",
        "y_true_con, y_prob_con = get_contrastive_predictions(valid_df)\n",
        "results[\"contrastive_bert\"] = {\n",
        "    \"y_true\": y_true_con,\n",
        "    \"y_prob\": y_prob_con,\n",
        "    \"metrics\": compute_classification_metrics(y_true_con, y_prob_con, threshold=0.5, label_names=LABELS),\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rows = []\n",
        "for name, info in results.items():\n",
        "    m = info[\"metrics\"]\n",
        "    rows.append({\n",
        "        \"model\": name,\n",
        "        \"macro_f1\": m[\"macro_f1\"],\n",
        "        \"micro_f1\": m[\"micro_f1\"],\n",
        "    })\n",
        "pd.DataFrame(rows).sort_values(\"macro_f1\", ascending=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see the expected hierarchy:\n",
        "\n",
        "- Lexicon-BERT and BERT achieve the highest macro F1 among all models.\n",
        "- DistilBERT is slightly behind BERT, but still clearly better than classical baselines.\n",
        "- Logistic Regression with TF–IDF outperforms plain BiLSTM, confirming that lexical cues are very strong for this dataset.\n",
        "- BiLSTM with attention narrows the gap but still trails Transformers.\n",
        "- The contrastive-BERT classifier performs significantly worse on macro F1. It is good at separating toxic vs non-toxic overall, but struggles with fine-grained labels, which we confirm below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "per_label_rows = []\n",
        "for name, info in results.items():\n",
        "    m = info[\"metrics\"][\"per_label\"]\n",
        "    for label in LABELS:\n",
        "        per_label_rows.append({\n",
        "            \"model\": name,\n",
        "            \"label\": label,\n",
        "            \"f1\": m[label][\"f1\"],\n",
        "            \"precision\": m[label][\"precision\"],\n",
        "            \"recall\": m[label][\"recall\"],\n",
        "        })\n",
        "per_label_df = pd.DataFrame(per_label_rows)\n",
        "per_label_df_pivot = per_label_df.pivot(index=\"label\", columns=\"model\", values=\"f1\")\n",
        "per_label_df_pivot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the easy label `toxic`, almost all models achieve strong F1.\n",
        "\n",
        "The more interesting story is in the rare labels:\n",
        "\n",
        "- `severe_toxic` and `threat` remain difficult across all models.\n",
        "- Lexicon-BERT tends to improve F1 for `severe_toxic`, `threat`, and `identity_hate` compared to vanilla BERT.\n",
        "- Contrastive-BERT collapses on `threat` and `identity_hate`, which supports the hypothesis that contrastive training at the level of \"toxic vs non-toxic\" is not well aligned with multi-label severity distinctions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label = \"threat\"\n",
        "label_idx = LABELS.index(label)\n",
        "model_names = [\"bert\", \"lexicon_bert\", \"contrastive_bert\"]\n",
        "n_examples = 5\n",
        "\n",
        "for name in model_names:\n",
        "    info = results[name]\n",
        "    y_true = info[\"y_true\"]\n",
        "    y_prob = info[\"y_prob\"]\n",
        "    fp, fn = get_fp_fn_indices(y_true, y_prob, label_idx, threshold=0.5)\n",
        "    \n",
        "    print(f\"\\n==== {name} - label: {label} - top {n_examples} false positives ====\")\n",
        "    for i in fp[:n_examples]:\n",
        "        row = valid_df.iloc[i]\n",
        "        print(f\"[prob={y_prob[i, label_idx]:.3f}] TRUE={int(y_true[i, label_idx])}\")\n",
        "        print(row[\"comment_text\"])\n",
        "        print(\"labels:\", row[LABELS].to_dict())\n",
        "        print(\"---\")\n",
        "    \n",
        "    print(f\"\\n==== {name} - label: {label} - top {n_examples} false negatives ====\")\n",
        "    for i in fn[:n_examples]:\n",
        "        row = valid_df.iloc[i]\n",
        "        print(f\"[prob={y_prob[i, label_idx]:.3f}] TRUE={int(y_true[i, label_idx])}\")\n",
        "        print(row[\"comment_text\"])\n",
        "        print(\"labels:\", row[LABELS].to_dict())\n",
        "        print(\"---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From these examples we can observe:\n",
        "\n",
        "- Many **false negatives** for `threat` correspond to implicit or indirect threats, such as \"you will regret this\" or \"wait until I see you in person\", which are difficult even for humans.\n",
        "- **Lexicon-BERT** tends to pick up explicit threat words such as \"kill\", \"beat\", \"destroy\", which explains its slightly higher recall on `threat`.\n",
        "- **Contrastive-BERT** often misses threats entirely or assigns low probability, confirming that the representation learned from generic toxic vs non-toxic pairs does not capture intensity or subtype information reliably.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "identity_terms = [\"gay\", \"muslim\", \"jew\", \"black\", \"asian\", \"christian\", \"women\", \"men\"]\n",
        "\n",
        "def contains_identity(text, terms):\n",
        "    text_low = text.lower()\n",
        "    return any(t in text_low for t in terms)\n",
        "\n",
        "subset_idx = [i for i, txt in enumerate(valid_df[\"comment_text\"].tolist()) if contains_identity(txt, identity_terms)]\n",
        "len(subset_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def inspect_identity_fp(model_name, label=\"identity_hate\", top_n=10):\n",
        "    label_idx = LABELS.index(label)\n",
        "    info = results[model_name]\n",
        "    y_true = info[\"y_true\"]\n",
        "    y_prob = info[\"y_prob\"]\n",
        "    \n",
        "    probs = y_prob[subset_idx, label_idx]\n",
        "    truths = y_true[subset_idx, label_idx]\n",
        "    idx_arr = np.array(subset_idx)\n",
        "    \n",
        "    # focus on false positives among identity mentions\n",
        "    fp_mask = (truths == 0) & (probs >= 0.5)\n",
        "    fp_idx = idx_arr[fp_mask]\n",
        "    order = np.argsort(probs[fp_mask])[::-1]\n",
        "    fp_idx = fp_idx[order]\n",
        "    \n",
        "    print(f\"\\nModel: {model_name} | identity-related false positives: {len(fp_idx)}\")\n",
        "    for i in fp_idx[:top_n]:\n",
        "        row = valid_df.iloc[i]\n",
        "        print(f\"[prob={y_prob[i, label_idx]:.3f}] TRUE={int(results[model_name]['y_true'][i, label_idx])}\")\n",
        "        print(row[\"comment_text\"])\n",
        "        print(\"labels:\", row[LABELS].to_dict())\n",
        "        print(\"---\")\n",
        "\n",
        "inspect_identity_fp(\"bert\", label=\"identity_hate\", top_n=5)\n",
        "inspect_identity_fp(\"lexicon_bert\", label=\"identity_hate\", top_n=5)\n",
        "inspect_identity_fp(\"contrastive_bert\", label=\"identity_hate\", top_n=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that:\n",
        "\n",
        "- All models still sometimes flag **neutral identity mentions** as identity-hate, especially in comments that contain mild disagreement plus a group reference.\n",
        "- Lexicon-BERT helps when slurs are explicit, but does not fully solve the bias problem.\n",
        "- Contrastive-BERT is unpredictable in identity-related contexts, reinforcing the need for a dedicated fairness analysis and possibly counterfactual augmentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "y_true_val = results[\"bert\"][\"y_true\"]  # same across models, but we pick one\n",
        "np.save(REPORTS_DIR / \"val_y_true.npy\", y_true_val)\n",
        "\n",
        "for name, info in results.items():\n",
        "    np.save(REPORTS_DIR / f\"val_{name}_probs.npy\", info[\"y_prob\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We saved:\n",
        "\n",
        "- `reports/val_y_true.npy`\n",
        "- `reports/val_<model>_probs.npy` for each model\n",
        "\n",
        "These will be used in a separate **ensemble and ablation** notebook.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
