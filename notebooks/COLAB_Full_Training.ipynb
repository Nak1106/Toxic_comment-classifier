{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸš€ Complete Training Pipeline for Toxic Comment Classification\n",
        "# Optimized for Google Colab GPU (15GB VRAM)\n",
        "\n",
        "This notebook runs all models for Phases 1-3:\n",
        "- Logistic Regression Baseline\n",
        "- BiLSTM\n",
        "- BiLSTM + Attention  \n",
        "- DistilBERT (faster)\n",
        "- BERT-base (best performance)\n",
        "\n",
        "**Runtime**: ~2-3 hours on Colab GPU\n",
        "**GPU Usage**: Optimized batch sizes and mixed precision training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Clone Repo and Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone your repo (replace with your actual repo URL)\n",
        "!git clone https://github.com/Nak1106/Toxic_comment-classifier.git\n",
        "%cd Toxic_comment-classifier\n",
        "\n",
        "# Install requirements\n",
        "!pip install -q torch transformers scikit-learn pandas matplotlib seaborn tqdm\n",
        "\n",
        "# Check GPU\n",
        "import torch\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload Dataset to Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Upload from local (run this cell and upload train.csv)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload your jigsaw train.csv\n",
        "!mv train.csv data/jigsaw_train.csv\n",
        "\n",
        "# Option 2: Download from Kaggle (if you have kaggle.json)\n",
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "# !kaggle competitions download -c jigsaw-toxic-comment-classification-challenge\n",
        "# !unzip jigsaw-toxic-comment-classification-challenge.zip -d data/\n",
        "# !mv data/train.csv data/jigsaw_train.csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Import project modules\n",
        "from src.config import DATA_DIR, MODELS_DIR, REPORTS_DIR, LABELS\n",
        "from src.data_utils import load_raw_jigsaw, train_valid_test_split, build_dataloaders_rnn, basic_text_clean\n",
        "from src.metrics import compute_classification_metrics\n",
        "from src.models.rnn_models import BiLSTMClassifier, BiLSTMAttentionClassifier\n",
        "from src.models.transformer_models import create_bert_base, create_distilbert\n",
        "\n",
        "# GPU optimization settings\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Memory-efficient settings for Colab\n",
        "BATCH_SIZE_RNN = 128      # RNN models are lightweight\n",
        "BATCH_SIZE_BERT = 16      # BERT needs more memory\n",
        "MAX_SEQ_LEN = 128         # Shorter sequences = faster + less memory\n",
        "USE_AMP = True            # Mixed precision training (2x faster on GPU)\n",
        "\n",
        "# Create directories\n",
        "MODELS_DIR.mkdir(exist_ok=True, parents=True)\n",
        "REPORTS_DIR.mkdir(exist_ok=True, parents=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Split Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading data...\")\n",
        "df = load_raw_jigsaw(DATA_DIR / \"jigsaw_train.csv\")\n",
        "train_df, valid_df, test_df = train_valid_test_split(df)\n",
        "\n",
        "print(f\"Train: {len(train_df)}, Valid: {len(valid_df)}, Test: {len(test_df)}\")\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(train_df[LABELS].sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 2: Logistic Regression Baseline (CPU)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"Training Logistic Regression Baseline...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Prepare text\n",
        "train_texts = [basic_text_clean(t) for t in train_df[\"comment_text\"].tolist()]\n",
        "valid_texts = [basic_text_clean(t) for t in valid_df[\"comment_text\"].tolist()]\n",
        "y_train = train_df[LABELS].values\n",
        "y_valid = valid_df[LABELS].values\n",
        "\n",
        "# TF-IDF\n",
        "print(\"Fitting TF-IDF...\")\n",
        "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 2), min_df=3)\n",
        "X_train_tfidf = tfidf.fit_transform(train_texts)\n",
        "X_valid_tfidf = tfidf.transform(valid_texts)\n",
        "\n",
        "# Train\n",
        "print(\"Training LogReg...\")\n",
        "logreg = MultiOutputClassifier(LogisticRegression(max_iter=100, C=4.0, solver='lbfgs', n_jobs=-1))\n",
        "logreg.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict\n",
        "y_prob_logreg = np.array([clf.predict_proba(X_valid_tfidf)[:, 1] for clf in logreg.estimators_]).T\n",
        "\n",
        "# Evaluate\n",
        "metrics_logreg = compute_classification_metrics(y_valid, y_prob_logreg, threshold=0.5, label_names=LABELS)\n",
        "\n",
        "# Save\n",
        "with open(REPORTS_DIR / \"logreg_baseline_metrics.json\", \"w\") as f:\n",
        "    json.dump(metrics_logreg, f, indent=2)\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\nâœ… LogReg Complete! Time: {elapsed/60:.1f} min\")\n",
        "print(f\"Macro F1: {metrics_logreg['macro_f1']:.4f}\")\n",
        "print(f\"Micro F1: {metrics_logreg['micro_f1']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 3A: BiLSTM Baseline (GPU)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"Training BiLSTM Baseline...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Build dataloaders\n",
        "train_loader, valid_loader, vocab = build_dataloaders_rnn(train_df, valid_df, max_len=100)\n",
        "\n",
        "# Model\n",
        "vocab_size = len(vocab)\n",
        "model_bilstm = BiLSTMClassifier(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=128,\n",
        "    hidden_dim=128,\n",
        "    num_labels=len(LABELS),\n",
        "    pad_idx=vocab[\"<pad>\"],\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model_bilstm.parameters(), lr=1e-3)\n",
        "\n",
        "# Training function\n",
        "def train_epoch_bilstm(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for x, y in tqdm(loader, desc=\"Training\"):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "# Eval function\n",
        "@torch.no_grad()\n",
        "def eval_epoch_bilstm(model, loader):\n",
        "    model.eval()\n",
        "    all_probs, all_labels = [], []\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        all_probs.append(probs.cpu().numpy())\n",
        "        all_labels.append(y.cpu().numpy())\n",
        "    return np.concatenate(all_labels, 0), np.concatenate(all_probs, 0)\n",
        "\n",
        "# Train for 5 epochs\n",
        "best_f1 = 0.0\n",
        "for epoch in range(1, 6):\n",
        "    train_loss = train_epoch_bilstm(model_bilstm, train_loader)\n",
        "    y_true, y_prob_bilstm = eval_epoch_bilstm(model_bilstm, valid_loader)\n",
        "    metrics_bilstm = compute_classification_metrics(y_true, y_prob_bilstm, threshold=0.5, label_names=LABELS)\n",
        "    \n",
        "    print(f\"Epoch {epoch} - Loss: {train_loss:.4f}, Macro F1: {metrics_bilstm['macro_f1']:.4f}\")\n",
        "    \n",
        "    if metrics_bilstm['macro_f1'] > best_f1:\n",
        "        best_f1 = metrics_bilstm['macro_f1']\n",
        "        torch.save({\"state_dict\": model_bilstm.state_dict(), \"vocab\": vocab}, \n",
        "                   MODELS_DIR / \"bilstm_baseline.pt\")\n",
        "        with open(REPORTS_DIR / \"bilstm_baseline_metrics.json\", \"w\") as f:\n",
        "            json.dump(metrics_bilstm, f, indent=2)\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\nâœ… BiLSTM Complete! Time: {elapsed/60:.1f} min\")\n",
        "print(f\"Best Macro F1: {best_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"Training BiLSTM + Attention...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Model with attention\n",
        "model_bilstm_attn = BiLSTMAttentionClassifier(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=128,\n",
        "    hidden_dim=128,\n",
        "    num_labels=len(LABELS),\n",
        "    pad_idx=vocab[\"<pad>\"],\n",
        ").to(device)\n",
        "\n",
        "optimizer_attn = torch.optim.Adam(model_bilstm_attn.parameters(), lr=1e-3)\n",
        "\n",
        "# Training function\n",
        "def train_epoch_attn(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for x, y in tqdm(loader, desc=\"Training\"):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer_attn.zero_grad()\n",
        "        logits, attn = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer_attn.step()\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "# Eval function\n",
        "@torch.no_grad()\n",
        "def eval_epoch_attn(model, loader):\n",
        "    model.eval()\n",
        "    all_probs, all_labels = [], []\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits, attn = model(x)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        all_probs.append(probs.cpu().numpy())\n",
        "        all_labels.append(y.cpu().numpy())\n",
        "    return np.concatenate(all_labels, 0), np.concatenate(all_probs, 0)\n",
        "\n",
        "# Train for 5 epochs\n",
        "best_f1_attn = 0.0\n",
        "for epoch in range(1, 6):\n",
        "    train_loss = train_epoch_attn(model_bilstm_attn, train_loader)\n",
        "    y_true, y_prob_attn = eval_epoch_attn(model_bilstm_attn, valid_loader)\n",
        "    metrics_attn = compute_classification_metrics(y_true, y_prob_attn, threshold=0.5, label_names=LABELS)\n",
        "    \n",
        "    print(f\"Epoch {epoch} - Loss: {train_loss:.4f}, Macro F1: {metrics_attn['macro_f1']:.4f}\")\n",
        "    \n",
        "    if metrics_attn['macro_f1'] > best_f1_attn:\n",
        "        best_f1_attn = metrics_attn['macro_f1']\n",
        "        torch.save({\"state_dict\": model_bilstm_attn.state_dict(), \"vocab\": vocab}, \n",
        "                   MODELS_DIR / \"bilstm_attention.pt\")\n",
        "        with open(REPORTS_DIR / \"bilstm_attention_metrics.json\", \"w\") as f:\n",
        "            json.dump(metrics_attn, f, indent=2)\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\nâœ… BiLSTM+Attention Complete! Time: {elapsed/60:.1f} min\")\n",
        "print(f\"Best Macro F1: {best_f1_attn:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 4: DistilBERT (GPU + Mixed Precision)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"Training DistilBERT (Fast + Memory Efficient)...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Clear GPU memory\n",
        "if device.type == \"cuda\":\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Prepare data for BERT\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class JigsawBertDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels.astype(\"float32\")\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        txt = str(self.texts[idx])\n",
        "        enc = self.tokenizer(txt, truncation=True, padding=\"max_length\",\n",
        "                           max_length=self.max_len, return_tensors=\"pt\")\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float32),\n",
        "        }\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "train_ds = JigsawBertDataset(train_df[\"comment_text\"].tolist(), train_df[LABELS].values,\n",
        "                              tokenizer, MAX_SEQ_LEN)\n",
        "valid_ds = JigsawBertDataset(valid_df[\"comment_text\"].tolist(), valid_df[LABELS].values,\n",
        "                              tokenizer, MAX_SEQ_LEN)\n",
        "\n",
        "train_loader_bert = DataLoader(train_ds, batch_size=BATCH_SIZE_BERT, shuffle=True, num_workers=2)\n",
        "valid_loader_bert = DataLoader(valid_ds, batch_size=BATCH_SIZE_BERT*2, shuffle=False, num_workers=2)\n",
        "\n",
        "# Model\n",
        "model_distilbert = create_distilbert(len(LABELS)).to(device)\n",
        "\n",
        "# Class weights for imbalanced labels\n",
        "pos_counts = train_df[LABELS].sum().values\n",
        "neg_counts = len(train_df) - pos_counts\n",
        "pos_weight = torch.tensor(neg_counts / np.maximum(pos_counts, 1), dtype=torch.float32).to(device)\n",
        "criterion_bert = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "optimizer_bert = torch.optim.AdamW(model_distilbert.parameters(), lr=2e-5)\n",
        "total_steps = len(train_loader_bert) * 3\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer_bert, num_warmup_steps=int(0.1*total_steps),\n",
        "                                           num_training_steps=total_steps)\n",
        "\n",
        "# Mixed precision scaler\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
        "\n",
        "# Training function with AMP\n",
        "def train_epoch_bert(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in tqdm(loader, desc=\"Training\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        \n",
        "        optimizer_bert.zero_grad()\n",
        "        \n",
        "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
        "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion_bert(logits, labels)\n",
        "        \n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer_bert)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "        \n",
        "        total_loss += loss.item() * input_ids.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "# Eval function\n",
        "@torch.no_grad()\n",
        "def eval_epoch_bert(model, loader):\n",
        "    model.eval()\n",
        "    all_probs, all_labels = [], []\n",
        "    for batch in loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        \n",
        "        logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        all_probs.append(probs.cpu().numpy())\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "    return np.concatenate(all_labels, 0), np.concatenate(all_probs, 0)\n",
        "\n",
        "# Train for 3 epochs (enough for DistilBERT)\n",
        "best_f1_distilbert = 0.0\n",
        "for epoch in range(1, 4):\n",
        "    train_loss = train_epoch_bert(model_distilbert, train_loader_bert)\n",
        "    y_true, y_prob_distilbert = eval_epoch_bert(model_distilbert, valid_loader_bert)\n",
        "    metrics_distilbert = compute_classification_metrics(y_true, y_prob_distilbert, threshold=0.5, label_names=LABELS)\n",
        "    \n",
        "    print(f\"Epoch {epoch} - Loss: {train_loss:.4f}, Macro F1: {metrics_distilbert['macro_f1']:.4f}\")\n",
        "    \n",
        "    if metrics_distilbert['macro_f1'] > best_f1_distilbert:\n",
        "        best_f1_distilbert = metrics_distilbert['macro_f1']\n",
        "        torch.save(model_distilbert.state_dict(), MODELS_DIR / \"distilbert_toxic.pt\")\n",
        "        with open(REPORTS_DIR / \"distilbert_toxic_metrics.json\", \"w\") as f:\n",
        "            json.dump(metrics_distilbert, f, indent=2)\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\nâœ… DistilBERT Complete! Time: {elapsed/60:.1f} min\")\n",
        "print(f\"Best Macro F1: {best_f1_distilbert:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"Training BERT-base (Best Performance)...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Clear GPU memory\n",
        "if device.type == \"cuda\":\n",
        "    torch.cuda.empty_cache()\n",
        "    del model_distilbert\n",
        "\n",
        "# Model\n",
        "model_bert = create_bert_base(len(LABELS)).to(device)\n",
        "optimizer_bert2 = torch.optim.AdamW(model_bert.parameters(), lr=2e-5)\n",
        "scheduler2 = get_linear_schedule_with_warmup(optimizer_bert2, num_warmup_steps=int(0.1*total_steps),\n",
        "                                            num_training_steps=total_steps)\n",
        "\n",
        "# Smaller batch size for BERT (more memory)\n",
        "BATCH_SIZE_BERT_LARGE = 8\n",
        "train_loader_bert_large = DataLoader(train_ds, batch_size=BATCH_SIZE_BERT_LARGE, shuffle=True, num_workers=2)\n",
        "valid_loader_bert_large = DataLoader(valid_ds, batch_size=BATCH_SIZE_BERT_LARGE*2, shuffle=False, num_workers=2)\n",
        "\n",
        "# Training function\n",
        "def train_epoch_bert_large(model, loader, optimizer, scheduler):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch in tqdm(loader, desc=\"Training\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        with torch.cuda.amp.autocast(enabled=USE_AMP):\n",
        "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion_bert(logits, labels)\n",
        "        \n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "        \n",
        "        total_loss += loss.item() * input_ids.size(0)\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "# Train for 3 epochs\n",
        "best_f1_bert = 0.0\n",
        "for epoch in range(1, 4):\n",
        "    train_loss = train_epoch_bert_large(model_bert, train_loader_bert_large, optimizer_bert2, scheduler2)\n",
        "    y_true, y_prob_bert = eval_epoch_bert(model_bert, valid_loader_bert_large)\n",
        "    metrics_bert = compute_classification_metrics(y_true, y_prob_bert, threshold=0.5, label_names=LABELS)\n",
        "    \n",
        "    print(f\"Epoch {epoch} - Loss: {train_loss:.4f}, Macro F1: {metrics_bert['macro_f1']:.4f}\")\n",
        "    \n",
        "    if metrics_bert['macro_f1'] > best_f1_bert:\n",
        "        best_f1_bert = metrics_bert['macro_f1']\n",
        "        torch.save(model_bert.state_dict(), MODELS_DIR / \"bert_toxic.pt\")\n",
        "        with open(REPORTS_DIR / \"bert_toxic_metrics.json\", \"w\") as f:\n",
        "            json.dump(metrics_bert, f, indent=2)\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\nâœ… BERT Complete! Time: {elapsed/60:.1f} min\")\n",
        "print(f\"Best Macro F1: {best_f1_bert:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š Final Results Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive comparison\n",
        "results = {\n",
        "    \"Logistic Regression\": metrics_logreg,\n",
        "    \"BiLSTM\": metrics_bilstm,\n",
        "    \"BiLSTM + Attention\": metrics_attn,\n",
        "    \"DistilBERT\": metrics_distilbert,\n",
        "    \"BERT-base\": metrics_bert,\n",
        "}\n",
        "\n",
        "# Overall comparison\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL RESULTS COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "comparison = []\n",
        "for model_name, metrics in results.items():\n",
        "    comparison.append({\n",
        "        \"Model\": model_name,\n",
        "        \"Macro F1\": f\"{metrics['macro_f1']:.4f}\",\n",
        "        \"Micro F1\": f\"{metrics['micro_f1']:.4f}\",\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Per-label comparison\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PER-LABEL F1 SCORES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "per_label_data = []\n",
        "for label in LABELS:\n",
        "    row = {\"Label\": label}\n",
        "    for model_name, metrics in results.items():\n",
        "        row[model_name] = f\"{metrics['per_label'][label]['f1']:.4f}\"\n",
        "    per_label_data.append(row)\n",
        "\n",
        "per_label_df = pd.DataFrame(per_label_data)\n",
        "print(per_label_df.to_string(index=False))\n",
        "\n",
        "# Visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Overall F1\n",
        "models = list(results.keys())\n",
        "macro_f1s = [results[m]['macro_f1'] for m in models]\n",
        "micro_f1s = [results[m]['micro_f1'] for m in models]\n",
        "\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "\n",
        "ax1.bar(x - width/2, macro_f1s, width, label='Macro F1', color='steelblue')\n",
        "ax1.bar(x + width/2, micro_f1s, width, label='Micro F1', color='coral')\n",
        "ax1.set_xlabel('Model')\n",
        "ax1.set_ylabel('F1 Score')\n",
        "ax1.set_title('Overall Performance Comparison')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(models, rotation=45, ha='right')\n",
        "ax1.legend()\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Per-label heatmap\n",
        "per_label_matrix = np.array([[results[m]['per_label'][l]['f1'] for l in LABELS] for m in models])\n",
        "im = ax2.imshow(per_label_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
        "ax2.set_xticks(np.arange(len(LABELS)))\n",
        "ax2.set_yticks(np.arange(len(models)))\n",
        "ax2.set_xticklabels(LABELS, rotation=45, ha='right')\n",
        "ax2.set_yticklabels(models)\n",
        "ax2.set_title('Per-Label F1 Scores Heatmap')\n",
        "\n",
        "# Add text annotations\n",
        "for i in range(len(models)):\n",
        "    for j in range(len(LABELS)):\n",
        "        text = ax2.text(j, i, f'{per_label_matrix[i, j]:.3f}',\n",
        "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
        "\n",
        "plt.colorbar(im, ax=ax2)\n",
        "plt.tight_layout()\n",
        "plt.savefig(REPORTS_DIR / \"model_comparison.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nâœ… All models trained! Results saved to: {REPORTS_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ’¾ Download Results from Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zip all results\n",
        "!zip -r results.zip models/ reports/\n",
        "\n",
        "# Download\n",
        "from google.colab import files\n",
        "files.download('results.zip')\n",
        "\n",
        "print(\"âœ… Download complete! Extract and commit to your repo.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
